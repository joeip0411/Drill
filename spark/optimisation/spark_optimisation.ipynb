{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "24/06/20 18:16:22 WARN Utils: Your hostname, DESKTOP-9INVMMS resolves to a loopback address: 127.0.1.1; using 172.29.67.34 instead (on interface eth0)\n",
      "24/06/20 18:16:22 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/joeip/Drill/.venv/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/joeip/.ivy2/cache\n",
      "The jars for the packages stored in: /home/joeip/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-common added as a dependency\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "com.amazonaws#aws-java-sdk-bundle added as a dependency\n",
      "org.apache.iceberg#iceberg-spark-runtime-3.5_2.12 added as a dependency\n",
      "org.apache.iceberg#iceberg-aws-bundle added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-f58d74cc-f7ed-4469-9106-5dc84e0977eb;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-common;3.3.4 in central\n",
      "\tfound org.apache.hadoop.thirdparty#hadoop-shaded-protobuf_3_7;1.1.1 in central\n",
      "\tfound org.apache.hadoop#hadoop-annotations;3.3.4 in central\n",
      "\tfound org.apache.hadoop.thirdparty#hadoop-shaded-guava;1.1.1 in central\n",
      "\tfound com.google.guava#guava;27.0-jre in central\n",
      "\tfound com.google.guava#failureaccess;1.0 in central\n",
      "\tfound com.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.2 in central\n",
      "\tfound org.checkerframework#checker-qual;2.5.2 in central\n",
      "\tfound com.google.j2objc#j2objc-annotations;1.1 in central\n",
      "\tfound org.codehaus.mojo#animal-sniffer-annotations;1.17 in central\n",
      "\tfound commons-cli#commons-cli;1.2 in central\n",
      "\tfound org.apache.commons#commons-math3;3.1.1 in central\n",
      "\tfound org.apache.httpcomponents#httpclient;4.5.13 in central\n",
      "\tfound org.apache.httpcomponents#httpcore;4.4.13 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound commons-codec#commons-codec;1.15 in central\n",
      "\tfound commons-io#commons-io;2.8.0 in central\n",
      "\tfound commons-net#commons-net;3.6 in central\n",
      "\tfound commons-collections#commons-collections;3.2.2 in central\n",
      "\tfound javax.servlet#javax.servlet-api;3.1.0 in central\n",
      "\tfound org.eclipse.jetty#jetty-server;9.4.43.v20210629 in central\n",
      "\tfound org.eclipse.jetty#jetty-http;9.4.43.v20210629 in central\n",
      "\tfound org.eclipse.jetty#jetty-util;9.4.43.v20210629 in central\n",
      "\tfound org.eclipse.jetty#jetty-io;9.4.43.v20210629 in central\n",
      "\tfound org.eclipse.jetty#jetty-servlet;9.4.43.v20210629 in central\n",
      "\tfound org.eclipse.jetty#jetty-security;9.4.43.v20210629 in central\n",
      "\tfound org.eclipse.jetty#jetty-util-ajax;9.4.43.v20210629 in central\n",
      "\tfound org.eclipse.jetty#jetty-webapp;9.4.43.v20210629 in central\n",
      "\tfound org.eclipse.jetty#jetty-xml;9.4.43.v20210629 in central\n",
      "\tfound com.sun.jersey#jersey-core;1.19 in central\n",
      "\tfound javax.ws.rs#jsr311-api;1.1.1 in central\n",
      "\tfound com.sun.jersey#jersey-servlet;1.19 in central\n",
      "\tfound com.sun.jersey#jersey-server;1.19 in central\n",
      "\tfound com.sun.jersey#jersey-json;1.19 in central\n",
      "\tfound org.codehaus.jettison#jettison;1.1 in central\n",
      "\tfound com.sun.xml.bind#jaxb-impl;2.2.3-1 in central\n",
      "\tfound javax.xml.bind#jaxb-api;2.2.11 in central\n",
      "\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in central\n",
      "\tfound org.codehaus.jackson#jackson-mapper-asl;1.9.13 in central\n",
      "\tfound org.codehaus.jackson#jackson-jaxrs;1.9.13 in central\n",
      "\tfound org.codehaus.jackson#jackson-xc;1.9.13 in central\n",
      "\tfound ch.qos.reload4j#reload4j;1.2.22 in central\n",
      "\tfound commons-beanutils#commons-beanutils;1.9.4 in central\n",
      "\tfound org.apache.commons#commons-configuration2;2.1.1 in central\n",
      "\tfound org.apache.commons#commons-lang3;3.12.0 in central\n",
      "\tfound org.apache.commons#commons-text;1.4 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.36 in central\n",
      "\tfound org.slf4j#slf4j-reload4j;1.7.36 in central\n",
      "\tfound org.apache.avro#avro;1.7.7 in central\n",
      "\tfound com.thoughtworks.paranamer#paranamer;2.3 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.2 in central\n",
      "\tfound org.apache.commons#commons-compress;1.21 in central\n",
      "\tfound com.google.re2j#re2j;1.1 in central\n",
      "\tfound com.google.protobuf#protobuf-java;2.5.0 in central\n",
      "\tfound com.google.code.gson#gson;2.8.9 in central\n",
      "\tfound org.apache.hadoop#hadoop-auth;3.3.4 in central\n",
      "\tfound com.nimbusds#nimbus-jose-jwt;9.8.1 in central\n",
      "\tfound com.github.stephenc.jcip#jcip-annotations;1.0-1 in central\n",
      "\tfound net.minidev#json-smart;2.4.7 in central\n",
      "\tfound net.minidev#accessors-smart;2.4.7 in central\n",
      "\tfound org.ow2.asm#asm;5.0.4 in central\n",
      "\tfound org.apache.zookeeper#zookeeper;3.5.6 in central\n",
      "\tfound org.apache.zookeeper#zookeeper-jute;3.5.6 in central\n",
      "\tfound org.apache.yetus#audience-annotations;0.5.0 in central\n",
      "\tfound org.apache.curator#curator-framework;4.2.0 in central\n",
      "\tfound org.apache.curator#curator-client;4.2.0 in central\n",
      "\tfound org.apache.kerby#kerb-simplekdc;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-client;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerby-config;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-core;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerby-pkix;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerby-asn1;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerby-util;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-common;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-crypto;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-util;1.0.1 in central\n",
      "\tfound org.apache.kerby#token-provider;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-admin;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-server;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerb-identity;1.0.1 in central\n",
      "\tfound org.apache.kerby#kerby-xdr;1.0.1 in central\n",
      "\tfound com.jcraft#jsch;0.1.55 in central\n",
      "\tfound org.apache.curator#curator-recipes;4.2.0 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-databind;2.12.7 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-annotations;2.12.7 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-core;2.12.7 in central\n",
      "\tfound org.codehaus.woodstox#stax2-api;4.2.1 in central\n",
      "\tfound com.fasterxml.woodstox#woodstox-core;5.3.0 in central\n",
      "\tfound dnsjava#dnsjava;2.1.7 in central\n",
      "\tfound jakarta.activation#jakarta.activation-api;1.2.1 in central\n",
      "\tfound javax.servlet.jsp#jsp-api;2.1 in central\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.4 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.12.262 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      "\tfound org.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.5.2 in central\n",
      "\tfound org.apache.iceberg#iceberg-aws-bundle;1.5.2 in central\n",
      ":: resolution report :: resolve 1305ms :: artifacts dl 65ms\n",
      "\t:: modules in use:\n",
      "\tch.qos.reload4j#reload4j;1.2.22 from central in [default]\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.262 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-annotations;2.12.7 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.12.7 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-databind;2.12.7 from central in [default]\n",
      "\tcom.fasterxml.woodstox#woodstox-core;5.3.0 from central in [default]\n",
      "\tcom.github.stephenc.jcip#jcip-annotations;1.0-1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.2 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.8.9 from central in [default]\n",
      "\tcom.google.guava#failureaccess;1.0 from central in [default]\n",
      "\tcom.google.guava#guava;27.0-jre from central in [default]\n",
      "\tcom.google.guava#listenablefuture;9999.0-empty-to-avoid-conflict-with-guava from central in [default]\n",
      "\tcom.google.j2objc#j2objc-annotations;1.1 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;2.5.0 from central in [default]\n",
      "\tcom.google.re2j#re2j;1.1 from central in [default]\n",
      "\tcom.jcraft#jsch;0.1.55 from central in [default]\n",
      "\tcom.nimbusds#nimbus-jose-jwt;9.8.1 from central in [default]\n",
      "\tcom.sun.jersey#jersey-core;1.19 from central in [default]\n",
      "\tcom.sun.jersey#jersey-json;1.19 from central in [default]\n",
      "\tcom.sun.jersey#jersey-server;1.19 from central in [default]\n",
      "\tcom.sun.jersey#jersey-servlet;1.19 from central in [default]\n",
      "\tcom.sun.xml.bind#jaxb-impl;2.2.3-1 from central in [default]\n",
      "\tcom.thoughtworks.paranamer#paranamer;2.3 from central in [default]\n",
      "\tcommons-beanutils#commons-beanutils;1.9.4 from central in [default]\n",
      "\tcommons-cli#commons-cli;1.2 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.15 from central in [default]\n",
      "\tcommons-collections#commons-collections;3.2.2 from central in [default]\n",
      "\tcommons-io#commons-io;2.8.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\tcommons-net#commons-net;3.6 from central in [default]\n",
      "\tdnsjava#dnsjava;2.1.7 from central in [default]\n",
      "\tjakarta.activation#jakarta.activation-api;1.2.1 from central in [default]\n",
      "\tjavax.servlet#javax.servlet-api;3.1.0 from central in [default]\n",
      "\tjavax.servlet.jsp#jsp-api;2.1 from central in [default]\n",
      "\tjavax.ws.rs#jsr311-api;1.1.1 from central in [default]\n",
      "\tjavax.xml.bind#jaxb-api;2.2.11 from central in [default]\n",
      "\tnet.minidev#accessors-smart;2.4.7 from central in [default]\n",
      "\tnet.minidev#json-smart;2.4.7 from central in [default]\n",
      "\torg.apache.avro#avro;1.7.7 from central in [default]\n",
      "\torg.apache.commons#commons-compress;1.21 from central in [default]\n",
      "\torg.apache.commons#commons-configuration2;2.1.1 from central in [default]\n",
      "\torg.apache.commons#commons-lang3;3.12.0 from central in [default]\n",
      "\torg.apache.commons#commons-math3;3.1.1 from central in [default]\n",
      "\torg.apache.commons#commons-text;1.4 from central in [default]\n",
      "\torg.apache.curator#curator-client;4.2.0 from central in [default]\n",
      "\torg.apache.curator#curator-framework;4.2.0 from central in [default]\n",
      "\torg.apache.curator#curator-recipes;4.2.0 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-annotations;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-auth;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-common;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop.thirdparty#hadoop-shaded-guava;1.1.1 from central in [default]\n",
      "\torg.apache.hadoop.thirdparty#hadoop-shaded-protobuf_3_7;1.1.1 from central in [default]\n",
      "\torg.apache.httpcomponents#httpclient;4.5.13 from central in [default]\n",
      "\torg.apache.httpcomponents#httpcore;4.4.13 from central in [default]\n",
      "\torg.apache.iceberg#iceberg-aws-bundle;1.5.2 from central in [default]\n",
      "\torg.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.5.2 from central in [default]\n",
      "\torg.apache.kerby#kerb-admin;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-client;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-common;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-core;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-crypto;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-identity;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-server;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-simplekdc;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerb-util;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerby-asn1;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerby-config;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerby-pkix;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerby-util;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#kerby-xdr;1.0.1 from central in [default]\n",
      "\torg.apache.kerby#token-provider;1.0.1 from central in [default]\n",
      "\torg.apache.yetus#audience-annotations;0.5.0 from central in [default]\n",
      "\torg.apache.zookeeper#zookeeper;3.5.6 from central in [default]\n",
      "\torg.apache.zookeeper#zookeeper-jute;3.5.6 from central in [default]\n",
      "\torg.checkerframework#checker-qual;2.5.2 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-core-asl;1.9.13 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-jaxrs;1.9.13 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-mapper-asl;1.9.13 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-xc;1.9.13 from central in [default]\n",
      "\torg.codehaus.jettison#jettison;1.1 from central in [default]\n",
      "\torg.codehaus.mojo#animal-sniffer-annotations;1.17 from central in [default]\n",
      "\torg.codehaus.woodstox#stax2-api;4.2.1 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-http;9.4.43.v20210629 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-io;9.4.43.v20210629 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-security;9.4.43.v20210629 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-server;9.4.43.v20210629 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-servlet;9.4.43.v20210629 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-util;9.4.43.v20210629 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-util-ajax;9.4.43.v20210629 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-webapp;9.4.43.v20210629 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-xml;9.4.43.v20210629 from central in [default]\n",
      "\torg.ow2.asm#asm;5.0.4 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.36 from central in [default]\n",
      "\torg.slf4j#slf4j-reload4j;1.7.36 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.2 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   97  |   0   |   0   |   0   ||   97  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-f58d74cc-f7ed-4469-9106-5dc84e0977eb\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 97 already retrieved (0kB/18ms)\n",
      "24/06/20 18:16:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/20 18:16:43 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "import os\n",
    "\n",
    "aws_access_key_id = os.getenv('AWS_ACCESS_KEY_ID')\n",
    "aws_secret_access_key = os.getenv('AWS_SECRET_ACCESS_KEY')\n",
    "catalog = 'glue'\n",
    "database = 'spark_drill'\n",
    "spark_conf = SparkConf()\n",
    "\n",
    "# https://github.com/apache/spark/blob/v3.5.1/pom.xml\n",
    "# https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-aws/3.3.4\n",
    "spark_conf.setAll(\n",
    "    [\n",
    "        ('spark.master', 'local[*]'),\n",
    "        ('spark.app.name', 'spark_app'),\n",
    "        # aws & iceberg\n",
    "        ('spark.jars.packages', 'org.apache.hadoop:hadoop-common:3.3.4,org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262,org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.2,org.apache.iceberg:iceberg-aws-bundle:1.5.2'),\n",
    "        # s3\n",
    "        ('spark.hadoop.fs.s3a.access.key', aws_access_key_id),\n",
    "        ('spark.hadoop.fs.s3a.secret.key', aws_secret_access_key),\n",
    "        ('spark.hadoop.fs.s3a.endpoint', 's3.amazonaws.com'),\n",
    "        ('spark.hadoop.fs.s3a.impl', 'org.apache.hadoop.fs.s3a.S3AFileSystem'),\n",
    "        # iceberg\n",
    "        ('spark.sql.extensions', 'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions'),\n",
    "        (f'spark.sql.catalog.{catalog}', 'org.apache.iceberg.spark.SparkCatalog'),\n",
    "        (f'spark.sql.catalog.{catalog}.catalog-impl', 'org.apache.iceberg.aws.glue.GlueCatalog'),\n",
    "        (f'spark.sql.catalog.{catalog}.warehouse', 's3://de-spark-practice/tpc-h/iceberg_table/'),\n",
    "        (f'spark.sqk.catalog.{catalog}.io-impl', 'org.apache.iceberg.aws.s3.S3FileIO')\n",
    "    ]\n",
    ")\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "    .config(conf=spark_conf)\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimtae_df_size(df) -> float:\n",
    "    df.cache()\n",
    "    nrows = df.count()\n",
    "    size_mb = spark._jvm.org.apache.spark.util.SizeEstimator.estimate(df._jdf)/ 1024**2\n",
    "    df.unpersist()\n",
    "\n",
    "    return size_mb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField, StructType, StringType, FloatType, TimestampType, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_schema = StructType([\n",
    "    StructField(name='cust_key', dataType=StringType(), nullable=True),\n",
    "    StructField(name='name', dataType=StringType(), nullable=True),\n",
    "    StructField(name='address', dataType=StringType(), nullable=True),\n",
    "    StructField(name='nation_key', dataType=StringType(), nullable=True),\n",
    "    StructField(name='phone', dataType=StringType(), nullable=True),\n",
    "    StructField(name='acct_bal', dataType=FloatType(), nullable=True),\n",
    "    StructField(name='mkt_segment', dataType=StringType(), nullable=True),\n",
    "    StructField(name='comment', dataType=StringType(), nullable=True),\n",
    "])\n",
    "customer = spark.read\\\n",
    "    .options(delimiter = '|',\n",
    "             header = False)\\\n",
    "    .schema(customer_schema)\\\n",
    "    .csv('s3a://de-spark-practice/tpc-h/raw/customer.tbl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_df_size_mb = estimtae_df_size(customer)\n",
    "print('customer: ', round(customer_df_size_mb,2), 'MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unprocessed\n",
    "customer.write\\\n",
    "    .format('iceberg')\\\n",
    "    .mode('overwrite')\\\n",
    "    .saveAsTable(f'{catalog}.{database}.customer_unprocessed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "customer table is small, can  coalesce into 1 single partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single partition \n",
    "customer\\\n",
    "    .repartition(1)\\\n",
    "    .write\\\n",
    "    .format('iceberg')\\\n",
    "    .mode('overwrite')\\\n",
    "    .saveAsTable(f'{catalog}.{database}.customer_single_partition')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f'''\n",
    "    SELECT \n",
    "        file_size_in_bytes/power(1024,2) as file_size_in_MB \n",
    "    FROM {catalog}.{database}.customer_single_partition.files;\n",
    "'''\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________\n",
    "Orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_schema = StructType([\n",
    "    StructField(name='order_key', dataType=StringType(), nullable=True),\n",
    "    StructField(name='cust_key', dataType=StringType(), nullable=True),\n",
    "    StructField(name='order_status', dataType=StringType(), nullable=True),\n",
    "    StructField(name='total_price', dataType=FloatType(), nullable=True),\n",
    "    StructField(name='order_date', dataType=TimestampType(), nullable=True),\n",
    "    StructField(name='order_priority', dataType=StringType(), nullable=True),\n",
    "    StructField(name='clerk', dataType=StringType(), nullable=True),\n",
    "    StructField(name='ship_priority', dataType=StringType(), nullable=True),\n",
    "    StructField(name='comment', dataType=StringType(), nullable=True),\n",
    "])\n",
    "\n",
    "orders = spark.read\\\n",
    "    .options(delimiter = '|',\n",
    "             header = False)\\\n",
    "    .schema(order_schema)\\\n",
    "    .csv('s3a://de-spark-practice/tpc-h/raw/orders.tbl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df_size_mb = estimtae_df_size(orders)\n",
    "print('orders: ', round(orders_df_size_mb,2), 'MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`order_key` is an continuous sequence, `arrow` will store this in the metadata instead of writing it as a physical column, to compare the storage size before/after sorting, we will hash the `order_key` with `md5`, and compare the size on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "order_unprocessed = orders.withColumn('order_surrogate_key', md5(col('order_key')))\\\n",
    "    .drop('order_key')\n",
    "\n",
    "order_unprocessed.write\\\n",
    "    .format('iceberg')\\\n",
    "    .mode('overwrite')\\\n",
    "    .saveAsTable(f'{catalog}.{database}.orders_unprocessed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by low cardinality column\n",
    "\n",
    "query = f'''\n",
    "    SELECT 'order_status' as col, count(distinct order_status) as cardinality from {catalog}.{database}.orders_unprocessed\n",
    "    union all\n",
    "    SELECT 'order_date' as col, count(distinct order_date) as cardinality from {catalog}.{database}.orders_unprocessed\n",
    "    union all\n",
    "    SELECT 'order_priority' as col, count(distinct order_priority) as cardinality from {catalog}.{database}.orders_unprocessed\n",
    "    union all\n",
    "    SELECT 'ship_priority' as col, count(distinct ship_priority) as cardinality from {catalog}.{database}.orders_unprocessed\n",
    "'''\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(query).explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using **order_status** as an example.\n",
    "- The table is not partitioned, Spark initialised 3 tasks to read the table, each task read in approximately the same number of records. ![alt text](order_status_task.jpg)\n",
    "- A local aggregation on *order_status* is performed. This will give the distinct order_status locally.\n",
    "- A shuffle happened to rearrange the records, records with same order_status will sit together in the same partition.\n",
    "- AQE coalesces the number of partition to 1 after shuffling\n",
    "- Another local aggregation on *order_status* is performed to remove duplicates\n",
    "- The partial count counts the number of distinct records in each partition\n",
    "- The last shuffle and aggregate make sure all the records are shuffle to the same partiton and a global aggregate is performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single partition, sort by columns in order of cardinality (low to high)\n",
    "order_sorted = order_unprocessed.repartition(1)\\\n",
    "    .orderBy('order_status', 'order_priority', 'order_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_sorted.write\\\n",
    "    .format('iceberg')\\\n",
    "    .mode('overwrite')\\\n",
    "    .saveAsTable(f'{catalog}.{database}.orders_single_partition_sorted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f'''\n",
    "    SELECT \n",
    "        'unsorted' as desc, round(sum(file_size_in_bytes/power(1024,2)),2) as file_size_in_MB \n",
    "    FROM {catalog}.{database}.orders_unprocessed.files\n",
    "\n",
    "    union all \n",
    "\n",
    "    SELECT \n",
    "        'sorted' , round(sum(file_size_in_bytes/power(1024,2)),2) \n",
    "    FROM {catalog}.{database}.orders_single_partition_sorted.files;\n",
    "'''\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sorted version is slightly better than the unsorted version.\n",
    "\n",
    "We will rewrite the table with the originl `order_key` to perform further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.write\\\n",
    "    .format('iceberg')\\\n",
    "    .mode('overwrite')\\\n",
    "    .saveAsTable(f'{catalog}.{database}.orders_unprocessed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.repartition(1)\\\n",
    "    .orderBy('order_status', 'order_priority', 'order_date')\\\n",
    "    .write\\\n",
    "    .format('iceberg')\\\n",
    "    .mode('overwrite')\\\n",
    "    .saveAsTable(f'{catalog}.{database}.orders_single_partition_sorted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f'''\n",
    "    SELECT \n",
    "        'unsorted' as desc, round(sum(file_size_in_bytes/power(1024,2)),2) as file_size_in_MB \n",
    "    FROM {catalog}.{database}.orders_unprocessed.files\n",
    "\n",
    "    union all \n",
    "\n",
    "    SELECT \n",
    "        'sorted' , round(sum(file_size_in_bytes/power(1024,2)),2) \n",
    "    FROM {catalog}.{database}.orders_single_partition_sorted.files;\n",
    "'''\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "customer & order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dafault shuffle partiton count\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# all time top 10 customer by spending\n",
    "unprocessed_file_execution_time = []\n",
    "\n",
    "for i in range(10):\n",
    "    unprocessed_query = f'''\n",
    "        with cte as (\n",
    "            select\n",
    "                c.name,\n",
    "                sum(total_price) as total_price,\n",
    "                row_number() over (partition by c.name order by sum(total_price)) as row_num\n",
    "            from {catalog}.{database}.customer_unprocessed c join {catalog}.{database}.orders_unprocessed o \n",
    "            on c.cust_key = o.cust_key\n",
    "            group by c.name\n",
    "        )\n",
    "\n",
    "        select\n",
    "            name,\n",
    "            total_price\n",
    "        from cte\n",
    "        where row_num <= 10\n",
    "        order by total_price desc;\n",
    "    '''\n",
    "    start_time = time.time()\n",
    "\n",
    "    spark.sql(unprocessed_query).collect()\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    unprocessed_file_execution_time.append(elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# all time top 10 customer by spending\n",
    "processed_file_execution_time = []\n",
    "for i in range(10):\n",
    "    processed_query = f'''\n",
    "        with cte as (\n",
    "            select\n",
    "                c.name,\n",
    "                sum(total_price) as total_price,\n",
    "                row_number() over (partition by c.name order by sum(total_price)) as row_num\n",
    "            from {catalog}.{database}.customer_single_partition c join {catalog}.{database}.orders_single_partition_sorted o \n",
    "            on c.cust_key = o.cust_key\n",
    "            group by c.name\n",
    "        )\n",
    "\n",
    "        select\n",
    "            name,\n",
    "            total_price\n",
    "        from cte\n",
    "        where row_num <= 10\n",
    "        order by total_price desc;\n",
    "    '''\n",
    "    start_time = time.time()\n",
    "\n",
    "    spark.sql(processed_query).collect()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    processed_file_execution_time.append(elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting shuffle partition to 1 for processed file\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# all time top 10 customer by spending\n",
    "processed_file_tuned_shuffle_partition_execution_time = []\n",
    "for i in range(10):\n",
    "    processed_query = f'''\n",
    "        with cte as (\n",
    "            select\n",
    "                c.name,\n",
    "                sum(total_price) as total_price,\n",
    "                row_number() over (partition by c.name order by sum(total_price)) as row_num\n",
    "            from {catalog}.{database}.customer_single_partition c join {catalog}.{database}.orders_single_partition_sorted o \n",
    "            on c.cust_key = o.cust_key\n",
    "            group by c.name\n",
    "        )\n",
    "\n",
    "        select\n",
    "            name,\n",
    "            total_price\n",
    "        from cte\n",
    "        where row_num <= 10\n",
    "        order by total_price desc;\n",
    "    '''\n",
    "    start_time = time.time()\n",
    "\n",
    "    spark.sql(processed_query).collect()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    processed_file_tuned_shuffle_partition_execution_time.append(elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "unprocessed_file_tuned_shuffle_partition_execution_time = []\n",
    "\n",
    "for i in range(10):\n",
    "    unprocessed_query = f'''\n",
    "        with cte as (\n",
    "            select\n",
    "                c.name,\n",
    "                sum(total_price) as total_price,\n",
    "                row_number() over (partition by c.name order by sum(total_price)) as row_num\n",
    "            from {catalog}.{database}.customer_unprocessed c join {catalog}.{database}.orders_unprocessed o \n",
    "            on c.cust_key = o.cust_key\n",
    "            group by c.name\n",
    "        )\n",
    "\n",
    "        select\n",
    "            name,\n",
    "            total_price\n",
    "        from cte\n",
    "        where row_num <= 10\n",
    "        order by total_price desc;\n",
    "    '''\n",
    "    start_time = time.time()\n",
    "\n",
    "    spark.sql(unprocessed_query).collect()\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    unprocessed_file_tuned_shuffle_partition_execution_time.append(elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "unprocessed_file_execution_time_df = pd.DataFrame(unprocessed_file_execution_time)\\\n",
    "    .reset_index()\\\n",
    "    .rename(columns = {0:'unprocessed_time'})\n",
    "processed_file_execution_time_df = pd.DataFrame(processed_file_execution_time)\\\n",
    "    .reset_index()\\\n",
    "    .rename(columns = {0:'processed_time'})\n",
    "processed_file_tuned_shuffle_partition_execution_time_df = pd.DataFrame(processed_file_tuned_shuffle_partition_execution_time)\\\n",
    "    .reset_index()\\\n",
    "    .rename(columns = {0:'processed_tuned_shuffle_partition_time'})\n",
    "unprocessed_file_tuned_shuffle_partition_execution_time_df = pd.DataFrame(unprocessed_file_tuned_shuffle_partition_execution_time)\\\n",
    "    .reset_index()\\\n",
    "    .rename(columns = {0:'tuned_shuffle_partition_time'})\n",
    "\n",
    "combine_execution_time_df = unprocessed_file_execution_time_df\\\n",
    "    .merge(processed_file_execution_time_df, on = 'index')\\\n",
    "    .merge(processed_file_tuned_shuffle_partition_execution_time_df, on = 'index')\\\n",
    "    .merge(unprocessed_file_tuned_shuffle_partition_execution_time_df, on='index')\\\n",
    "    .rename(columns={'index':'iteration'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iteration</th>\n",
       "      <th>unprocessed_time</th>\n",
       "      <th>processed_time</th>\n",
       "      <th>processed_tuned_shuffle_partition_time</th>\n",
       "      <th>tuned_shuffle_partition_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>6.597296</td>\n",
       "      <td>4.390409</td>\n",
       "      <td>3.133622</td>\n",
       "      <td>5.283380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4.458177</td>\n",
       "      <td>3.692109</td>\n",
       "      <td>3.119999</td>\n",
       "      <td>4.169274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5.313872</td>\n",
       "      <td>4.038579</td>\n",
       "      <td>3.044740</td>\n",
       "      <td>5.348646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4.392641</td>\n",
       "      <td>3.247443</td>\n",
       "      <td>3.273716</td>\n",
       "      <td>4.352708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4.595046</td>\n",
       "      <td>3.264681</td>\n",
       "      <td>3.071440</td>\n",
       "      <td>4.671934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>4.126972</td>\n",
       "      <td>3.268821</td>\n",
       "      <td>3.215186</td>\n",
       "      <td>4.797653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>4.913410</td>\n",
       "      <td>3.140642</td>\n",
       "      <td>3.474682</td>\n",
       "      <td>4.682474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>4.642701</td>\n",
       "      <td>3.223811</td>\n",
       "      <td>4.720748</td>\n",
       "      <td>3.828924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>5.243189</td>\n",
       "      <td>3.205679</td>\n",
       "      <td>3.058208</td>\n",
       "      <td>4.179294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>5.937499</td>\n",
       "      <td>3.074231</td>\n",
       "      <td>3.113074</td>\n",
       "      <td>4.465204</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   iteration  unprocessed_time  processed_time  \\\n",
       "0          0          6.597296        4.390409   \n",
       "1          1          4.458177        3.692109   \n",
       "2          2          5.313872        4.038579   \n",
       "3          3          4.392641        3.247443   \n",
       "4          4          4.595046        3.264681   \n",
       "5          5          4.126972        3.268821   \n",
       "6          6          4.913410        3.140642   \n",
       "7          7          4.642701        3.223811   \n",
       "8          8          5.243189        3.205679   \n",
       "9          9          5.937499        3.074231   \n",
       "\n",
       "   processed_tuned_shuffle_partition_time  tuned_shuffle_partition_time  \n",
       "0                                3.133622                      5.283380  \n",
       "1                                3.119999                      4.169274  \n",
       "2                                3.044740                      5.348646  \n",
       "3                                3.273716                      4.352708  \n",
       "4                                3.071440                      4.671934  \n",
       "5                                3.215186                      4.797653  \n",
       "6                                3.474682                      4.682474  \n",
       "7                                4.720748                      3.828924  \n",
       "8                                3.058208                      4.179294  \n",
       "9                                3.113074                      4.465204  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(combine_execution_time_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unrpocessed file:  5.022080206871033\n",
      "unprocessed file & tuned shuffle partition:  4.5779492378234865\n",
      "processed file:  3.4546404838562013\n",
      "processed file & tuned shuffle partition:  3.3225415468215944\n"
     ]
    }
   ],
   "source": [
    "print('unrpocessed file: ', combine_execution_time_df['unprocessed_time'].mean())\n",
    "print('unprocessed file & tuned shuffle partition: ', combine_execution_time_df['tuned_shuffle_partition_time'].mean())\n",
    "print('processed file: ', combine_execution_time_df['processed_time'].mean())\n",
    "print('processed file & tuned shuffle partition: ', combine_execution_time_df['processed_tuned_shuffle_partition_time'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The query execution time for single partition table is ~33% faster than table with small files. Tuning the shuffle partition only provides a marginal benefit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____________\n",
    "line item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_schema = StructType([\n",
    "    StructField(name='order_key', dataType=StringType(), nullable=True),\n",
    "    StructField(name='part_key', dataType=StringType(), nullable=True),\n",
    "    StructField(name='supp_key', dataType=StringType(), nullable=True),\n",
    "    StructField(name='line_number', dataType=IntegerType(), nullable=True),\n",
    "    StructField(name='quantity', dataType=IntegerType(), nullable=True),\n",
    "    StructField(name='extended_price', dataType=FloatType(), nullable=True),\n",
    "    StructField(name='discount', dataType=FloatType(), nullable=True),\n",
    "    StructField(name='tax', dataType=FloatType(), nullable=True),\n",
    "    StructField(name='return_flag', dataType=StringType(), nullable=True),\n",
    "    StructField(name='line_status', dataType=StringType(), nullable=True),\n",
    "    StructField(name='ship_date', dataType=TimestampType(), nullable=True),\n",
    "    StructField(name='commit_date', dataType=TimestampType(), nullable=True),\n",
    "    StructField(name='recipt_date', dataType=TimestampType(), nullable=True),\n",
    "    StructField(name='ship_instruct', dataType=StringType(), nullable=True),\n",
    "    StructField(name='ship_mode', dataType=StringType(), nullable=True),\n",
    "    StructField(name='comment', dataType=StringType(), nullable=True),\n",
    "])\n",
    "line_item = spark.read\\\n",
    "    .options(delimiter = '|',\n",
    "             header = False)\\\n",
    "    .schema(customer_schema)\\\n",
    "    .csv('s3a://de-spark-practice/tpc-h/raw/lineitem.tbl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "line_item.write\\\n",
    "    .format('iceberg')\\\n",
    "    .mode('overwrite')\\\n",
    "    .saveAsTable(f'{catalog}.{database}.line_item_unprocessed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "line_item.repartition(1)\\\n",
    "    .write\\\n",
    "    .format('iceberg')\\\n",
    "    .mode('overwrite')\\\n",
    "    .saveAsTable(f'{catalog}.{database}.line_item_single_partition')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________\n",
    "Most expensive order line items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/20 22:53:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/06/20 22:53:50 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/06/20 22:53:55 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/06/20 22:53:55 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/06/20 22:53:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/06/20 22:53:57 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+--------+\n",
      "|order_key|total_price|part_key|\n",
      "+---------+-----------+--------+\n",
      "|  2159139|     857.71|   32021|\n",
      "+---------+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f'''\n",
    "\n",
    "    with cte as (\n",
    "        select\n",
    "        o.order_key,\n",
    "        o.total_price,\n",
    "        dense_rank() over(order by o.total_price) as price_rank\n",
    "    from {catalog}.{database}.orders_single_partition_sorted o\n",
    "    )\n",
    "    \n",
    "    select\n",
    "        c.order_key,\n",
    "        c.total_price,\n",
    "        l.part_key\n",
    "    from cte c join {catalog}.{database}.line_item_single_partition l on c.order_key = l.order_key\n",
    "    where c.price_rank = 1\n",
    "    \n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/20 22:51:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/06/20 22:51:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/06/20 22:51:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/06/20 22:51:31 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+--------+\n",
      "|order_key|total_price|part_key|\n",
      "+---------+-----------+--------+\n",
      "|  2159139|     857.71|   32021|\n",
      "+---------+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f'''\n",
    "\n",
    "    with cte as (\n",
    "        select\n",
    "        o.order_key,\n",
    "        o.total_price,\n",
    "        dense_rank() over(order by o.total_price) as price_rank\n",
    "    from {catalog}.{database}.orders_unprocessed o\n",
    "    )\n",
    "    \n",
    "    select\n",
    "        c.order_key,\n",
    "        c.total_price,\n",
    "        l.part_key\n",
    "    from cte c join {catalog}.{database}.line_item_unprocessed l on c.order_key = l.order_key\n",
    "    where c.price_rank = 1\n",
    "    \n",
    "''').show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workspace_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
