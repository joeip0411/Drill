{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "import os\n",
    "\n",
    "aws_access_key_id = os.getenv('AWS_ACCESS_KEY_ID')\n",
    "aws_secret_access_key = os.getenv('AWS_SECRET_ACCESS_KEY')\n",
    "catalog = 'glue'\n",
    "database = 'spark_drill'\n",
    "spark_conf = SparkConf()\n",
    "\n",
    "# https://github.com/apache/spark/blob/v3.5.1/pom.xml\n",
    "# https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-aws/3.3.4\n",
    "spark_conf.setAll(\n",
    "    [\n",
    "        ('spark.master', 'local[*]'),\n",
    "        ('spark.app.name', 'spark_app'),\n",
    "        # aws\n",
    "        ('spark.jars.packages', 'org.apache.hadoop:hadoop-common:3.3.4,org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262,org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.2,org.apache.iceberg:iceberg-aws-bundle:1.5.2'),\n",
    "        # s3\n",
    "        ('spark.hadoop.fs.s3a.access.key', aws_access_key_id),\n",
    "        ('spark.hadoop.fs.s3a.secret.key', aws_secret_access_key),\n",
    "        ('spark.hadoop.fs.s3a.endpoint', 's3.amazonaws.com'),\n",
    "        ('spark.hadoop.fs.s3a.impl', 'org.apache.hadoop.fs.s3a.S3AFileSystem'),\n",
    "        # iceberg\n",
    "        ('spark.sql.extensions', 'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions'),\n",
    "        (f'spark.sql.catalog.{catalog}', 'org.apache.iceberg.spark.SparkCatalog'),\n",
    "        (f'spark.sql.catalog.{catalog}.catalog-impl', 'org.apache.iceberg.aws.glue.GlueCatalog'),\n",
    "        (f'spark.sql.catalog.{catalog}.warehouse', 's3://de-spark-practice/tpc-h/iceberg_table/'),\n",
    "        (f'spark.sqk.catalog.{catalog}.io-impl', 'org.apache.iceberg.aws.s3.S3FileIO')\n",
    "    ]\n",
    ")\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "    .config(conf=spark_conf)\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimtae_df_size(df) -> float:\n",
    "    df.cache()\n",
    "    nrows = df.count()\n",
    "    size_mb = spark._jvm.org.apache.spark.util.SizeEstimator.estimate(df._jdf)/ 1024**2\n",
    "    df.unpersist()\n",
    "\n",
    "    return size_mb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/14 16:09:49 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/14 16:09:50 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructField, StructType, StringType, FloatType, TimestampType\n",
    "\n",
    "customer_schema = StructType([\n",
    "    StructField(name='cust_key', dataType=StringType(), nullable=True),\n",
    "    StructField(name='name', dataType=StringType(), nullable=True),\n",
    "    StructField(name='address', dataType=StringType(), nullable=True),\n",
    "    StructField(name='nation_key', dataType=StringType(), nullable=True),\n",
    "    StructField(name='phone', dataType=StringType(), nullable=True),\n",
    "    StructField(name='acct_bal', dataType=FloatType(), nullable=True),\n",
    "    StructField(name='mkt_segment', dataType=StringType(), nullable=True),\n",
    "    StructField(name='comment', dataType=StringType(), nullable=True),\n",
    "])\n",
    "customer = spark.read\\\n",
    "    .options(delimiter = '|',\n",
    "             header = False)\\\n",
    "    .schema(customer_schema)\\\n",
    "    .csv('s3a://de-spark-practice/tpc-h/raw/customer.tbl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "customer:  28.8 MB\n"
     ]
    }
   ],
   "source": [
    "customer_df_size_mb = estimtae_df_size(customer)\n",
    "print('customer: ', round(customer_df_size_mb,2), 'MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# unprocessed\n",
    "customer.write\\\n",
    "    .format('iceberg')\\\n",
    "    .mode('overwrite')\\\n",
    "    .saveAsTable(f'{catalog}.{database}.customer_unprocessed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "customer table is small, can  coalesce into 1 single partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# single partition \n",
    "customer\\\n",
    "    .repartition(1)\\\n",
    "    .write\\\n",
    "    .format('iceberg')\\\n",
    "    .mode('overwrite')\\\n",
    "    .saveAsTable(f'{catalog}.{database}.customer_single_partition')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|  file_size_in_MB|\n",
      "+-----------------+\n",
      "|7.593967437744141|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = f'''\n",
    "    SELECT \n",
    "        file_size_in_bytes/power(1024,2) as file_size_in_MB \n",
    "    FROM {catalog}.{database}.customer_single_partition.files;\n",
    "'''\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_schema = StructType([\n",
    "    StructField(name='order_key', dataType=StringType(), nullable=True),\n",
    "    StructField(name='cust_key', dataType=StringType(), nullable=True),\n",
    "    StructField(name='order_status', dataType=StringType(), nullable=True),\n",
    "    StructField(name='total_price', dataType=FloatType(), nullable=True),\n",
    "    StructField(name='order_date', dataType=TimestampType(), nullable=True),\n",
    "    StructField(name='order_priority', dataType=StringType(), nullable=True),\n",
    "    StructField(name='clerk', dataType=StringType(), nullable=True),\n",
    "    StructField(name='ship_priority', dataType=StringType(), nullable=True),\n",
    "    StructField(name='comment', dataType=StringType(), nullable=True),\n",
    "])\n",
    "\n",
    "orders = spark.read\\\n",
    "    .options(delimiter = '|',\n",
    "             header = False)\\\n",
    "    .schema(order_schema)\\\n",
    "    .csv('s3a://de-spark-practice/tpc-h/raw/orders.tbl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:====================================================>    (11 + 1) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orders:  134.61 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "orders_df_size_mb = estimtae_df_size(orders)\n",
    "print('orders: ', round(orders_df_size_mb,2), 'MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# unprocessed\n",
    "orders.write\\\n",
    "    .format('iceberg')\\\n",
    "    .mode('overwrite')\\\n",
    "    .saveAsTable(f'{catalog}.{database}.orders_unprocessed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 69:>                 (0 + 3) / 3][Stage 71:============>     (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+\n",
      "|           col|cardinality|\n",
      "+--------------+-----------+\n",
      "|  order_status|          3|\n",
      "|    order_date|       2406|\n",
      "|order_priority|          5|\n",
      "| ship_priority|          1|\n",
      "+--------------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# sort by low cardinality column\n",
    "\n",
    "query = f'''\n",
    "    SELECT 'order_status' as col, count(distinct order_status) as cardinality from {catalog}.{database}.orders_unprocessed\n",
    "    union all\n",
    "    SELECT 'order_date' as col, count(distinct order_date) as cardinality from {catalog}.{database}.orders_unprocessed\n",
    "    union all\n",
    "    SELECT 'order_priority' as col, count(distinct order_priority) as cardinality from {catalog}.{database}.orders_unprocessed\n",
    "    union all\n",
    "    SELECT 'ship_priority' as col, count(distinct ship_priority) as cardinality from {catalog}.{database}.orders_unprocessed\n",
    "'''\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Union\n",
      "   :- HashAggregate(keys=[], functions=[count(distinct order_status#1467)])\n",
      "   :  +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=948]\n",
      "   :     +- HashAggregate(keys=[], functions=[partial_count(distinct order_status#1467)])\n",
      "   :        +- HashAggregate(keys=[order_status#1467], functions=[])\n",
      "   :           +- Exchange hashpartitioning(order_status#1467, 200), ENSURE_REQUIREMENTS, [plan_id=944]\n",
      "   :              +- HashAggregate(keys=[order_status#1467], functions=[])\n",
      "   :                 +- BatchScan glue.spark_drill.orders_unprocessed[order_status#1467] glue.spark_drill.orders_unprocessed (branch=null) [filters=, groupedBy=] RuntimeFilters: []\n",
      "   :- HashAggregate(keys=[], functions=[count(distinct order_date#1478)])\n",
      "   :  +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=954]\n",
      "   :     +- HashAggregate(keys=[], functions=[partial_count(distinct order_date#1478)])\n",
      "   :        +- HashAggregate(keys=[order_date#1478], functions=[])\n",
      "   :           +- Exchange hashpartitioning(order_date#1478, 200), ENSURE_REQUIREMENTS, [plan_id=950]\n",
      "   :              +- HashAggregate(keys=[order_date#1478], functions=[])\n",
      "   :                 +- BatchScan glue.spark_drill.orders_unprocessed[order_date#1478] glue.spark_drill.orders_unprocessed (branch=null) [filters=, groupedBy=] RuntimeFilters: []\n",
      "   :- HashAggregate(keys=[], functions=[count(distinct order_priority#1488)])\n",
      "   :  +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=960]\n",
      "   :     +- HashAggregate(keys=[], functions=[partial_count(distinct order_priority#1488)])\n",
      "   :        +- HashAggregate(keys=[order_priority#1488], functions=[])\n",
      "   :           +- Exchange hashpartitioning(order_priority#1488, 200), ENSURE_REQUIREMENTS, [plan_id=956]\n",
      "   :              +- HashAggregate(keys=[order_priority#1488], functions=[])\n",
      "   :                 +- BatchScan glue.spark_drill.orders_unprocessed[order_priority#1488] glue.spark_drill.orders_unprocessed (branch=null) [filters=, groupedBy=] RuntimeFilters: []\n",
      "   +- HashAggregate(keys=[], functions=[count(distinct ship_priority#1499)])\n",
      "      +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [plan_id=966]\n",
      "         +- HashAggregate(keys=[], functions=[partial_count(distinct ship_priority#1499)])\n",
      "            +- HashAggregate(keys=[ship_priority#1499], functions=[])\n",
      "               +- Exchange hashpartitioning(ship_priority#1499, 200), ENSURE_REQUIREMENTS, [plan_id=962]\n",
      "                  +- HashAggregate(keys=[ship_priority#1499], functions=[])\n",
      "                     +- BatchScan glue.spark_drill.orders_unprocessed[ship_priority#1499] glue.spark_drill.orders_unprocessed (branch=null) [filters=, groupedBy=] RuntimeFilters: []\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(query).explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using **order_status** as an example.\n",
    "- The table is not partitioned, Spark initialised 3 tasks to read the table, each task read in approximately the same number of records. ![alt text](order_status_task.jpg)\n",
    "- A local aggregation on *order_status* is performed. This will give the distinct order_status locally.\n",
    "- A shuffle happened to rearrange the records, records with same order_status will sit together in the same partition.\n",
    "- AQE coalesces the number of partition to 1 after shuffling\n",
    "- Another local aggregation on *order_status* is performed to remove duplicates\n",
    "- The partial count counts the number of distinct records\n",
    "- The last shuffle and aggregate is redundant since AQE has already coleasced the number of partition to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# single partition, sort by 'order_status', 'order_priority', 'order_date'\n",
    "orders.repartition(1)\\\n",
    "    .sortWithinPartitions('order_status', 'order_priority', 'order_date')\\\n",
    "    .write\\\n",
    "    .format('iceberg')\\\n",
    "    .mode('overwrite')\\\n",
    "    .saveAsTable(f'{catalog}.{database}.orders_single_partition_sorted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+\n",
      "|    desc|file_size_in_MB|\n",
      "+--------+---------------+\n",
      "|unsorted|          34.97|\n",
      "|  sorted|          36.72|\n",
      "+--------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = f'''\n",
    "    SELECT \n",
    "        'unsorted' as desc, round(sum(file_size_in_bytes/power(1024,2)),2) as file_size_in_MB \n",
    "    FROM {catalog}.{database}.orders_unprocessed.files\n",
    "\n",
    "    union all \n",
    "\n",
    "    SELECT \n",
    "        'sorted' , round(sum(file_size_in_bytes/power(1024,2)),2) \n",
    "    FROM {catalog}.{database}.orders_single_partition_sorted.files;\n",
    "'''\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+----------------------+---------------------+-----------------------+\n",
      "|rewritten_data_files_count|added_data_files_count|rewritten_bytes_count|failed_data_files_count|\n",
      "+--------------------------+----------------------+---------------------+-----------------------+\n",
      "|                         3|                     1|             38501568|                      0|\n",
      "+--------------------------+----------------------+---------------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = f'''\n",
    "    CALL glue.system.rewrite_data_files(\n",
    "    table => '{database}.orders_single_partition_sorted', \n",
    "    strategy => 'sort', \n",
    "    options => map('min-input-files','1'),\n",
    "    sort_order => 'order_status ASC,  order_priority ASC, order_date ASC' \n",
    "    )\n",
    "'''\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+\n",
      "|    desc|file_size_in_MB|\n",
      "+--------+---------------+\n",
      "|unsorted|          34.97|\n",
      "|  sorted|          36.63|\n",
      "+--------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = f'''\n",
    "    SELECT \n",
    "        'unsorted' as desc, round(sum(file_size_in_bytes/power(1024,2)),2) as file_size_in_MB \n",
    "    FROM {catalog}.{database}.orders_unprocessed.files\n",
    "\n",
    "    union all \n",
    "\n",
    "    SELECT \n",
    "        'sorted' , round(sum(file_size_in_bytes/power(1024,2)),2) \n",
    "    FROM {catalog}.{database}.orders_single_partition_sorted.files;\n",
    "'''\n",
    "spark.sql(query).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workspace_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
